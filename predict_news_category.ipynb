{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/isabelcorpus/.pyenv/versions/3.9.0/lib/python3.9/site-packages')\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from analysis_helper import *\n",
    "import torch\n",
    "import json\n",
    "import seaborn as sns\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we complete the following tasks: \n",
    "- Load data from .vec and .tsv files \n",
    "- Process data for use \n",
    "- Create mean pooled Wikidata Knowledge Graph entity embeddings (100d) at the news item level \n",
    "- Generate news title embeddings using pretrained SBERT model (384d)\n",
    "- Predict news category through three simple networks of similar architecture, such that we may compare performance when using entity embeddings, sbert embeddings, and a concatenation of both sources as input (484d).\n",
    "\n",
    "I find that the test accuracy for the third case has the best performance, implying that there is information to be gained beyond the sbert sentence embeddings. In future work, it would be interesting to explore performance by category or the linguistic features/Wiki entities that contribute most heavily to each prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabelcorpus/sample_project/mind/analysis_helper/preprocessing.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col_i].fillna('{}', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data: \n",
    "# load files, normalize embeddings, preprocess for analysis and modeling \n",
    "news = pd.read_csv(\"data/MINDsmall_train/news.tsv\", sep='\\t', \n",
    "                   names = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"])\n",
    "news = process_tsv(news, ['title_entities', 'abstract_entities'])\n",
    "\n",
    "entity_id, entity_vec = load_embeddings('data/MINDsmall_train/entity_embedding.vec')\n",
    "relation_id, relation_vec = load_embeddings('data/MINDsmall_train/relation_embedding.vec')\n",
    "\n",
    "entity_vec = normalize(entity_vec)\n",
    "\n",
    "# extract WikiData Knowledge Graph entity IDs for title and abstract\n",
    "news['title_entity_ids'] = extract_entity_list(news, 'title_entities', 'WikidataId')\n",
    "news['abstract_entity_ids'] = extract_entity_list(news, 'abstract_entities', 'WikidataId')\n",
    "\n",
    "# mean pool entity embeddings to create news embeddings  \n",
    "news_embeddings = mean_pooled_news_embeddings(entity_vec, entity_id, news, \"title_entity_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence-bert embeddings using pretrained SBERT model \n",
    "# load pre-trained sbert \n",
    "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "title_embeddings = sbert.encode(np.vstack(news.title).flatten())\n",
    "\n",
    "# join embeddings with news metadata \n",
    "title_embedding_df = pd.DataFrame(zip(news.news_id, title_embeddings), columns = [\"news_id\", \"title_embedding\"])\n",
    "news_title_entity = pd.merge(title_embedding_df, news_embeddings, how = \"inner\", on = \"news_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode category field as integer, create Dataset and load into DataLoader for training \n",
    "news_embeddings['Numerical_Category'] = cat_to_int(news_embeddings, \"category\")\n",
    "\n",
    "# assign weights to each class to account for imbalance in loss fn \n",
    "weights = torch.tensor(list(1 - news_embeddings.Numerical_Category.value_counts().sort_index()/len(news_embeddings)))\n",
    "\n",
    "# create Dataset for which each X - pooled entity embeddings, y - category of news\n",
    "news_dataset = NewsDataset(np.vstack(news_embeddings.entity_vec), news_embeddings['Numerical_Category'])\n",
    "\n",
    "# split 0.8 training, 0.2 test \n",
    "train_dataset, test_dataset = test_training_split(news_dataset, 0.8)\n",
    "loader = DataLoader(train_dataset, shuffle=True, batch_size = 10)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 6918.407646417618, training accuracy: 0.5401629209518433\n",
      "epoch: 1, training loss: 6715.028743863106, training accuracy: 0.6223741173744202\n",
      "epoch: 2, training loss: 6670.079519748688, training accuracy: 0.6316994428634644\n",
      "epoch: 3, training loss: 6638.904370546341, training accuracy: 0.6391395330429077\n",
      "epoch: 4, training loss: 6616.911562800407, training accuracy: 0.6447616219520569\n",
      "epoch: 5, training loss: 6596.905938267708, training accuracy: 0.6483638286590576\n",
      "epoch: 6, training loss: 6578.543019890785, training accuracy: 0.6502491235733032\n",
      "epoch: 7, training loss: 6566.764093637466, training accuracy: 0.6522690653800964\n",
      "epoch: 8, training loss: 6552.1642454862595, training accuracy: 0.6546593308448792\n",
      "epoch: 9, training loss: 6540.041749000549, training accuracy: 0.6555683016777039\n",
      "epoch: 10, training loss: 6530.636615276337, training accuracy: 0.6564099192619324\n",
      "epoch: 11, training loss: 6520.210317611694, training accuracy: 0.6570832133293152\n",
      "epoch: 12, training loss: 6509.806337594986, training accuracy: 0.6585308313369751\n",
      "epoch: 13, training loss: 6500.829562187195, training accuracy: 0.6594398021697998\n",
      "epoch: 14, training loss: 6491.6199679374695, training accuracy: 0.6595744490623474\n",
      "epoch: 15, training loss: 6482.943824052811, training accuracy: 0.6603150963783264\n",
      "epoch: 16, training loss: 6475.210930228233, training accuracy: 0.6614260673522949\n",
      "epoch: 17, training loss: 6467.019938707352, training accuracy: 0.664152979850769\n",
      "epoch: 18, training loss: 6460.657493114471, training accuracy: 0.6642203330993652\n",
      "epoch: 19, training loss: 6452.523126602173, training accuracy: 0.6645906567573547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6478), 16258.88407254219)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple sequential NN to predict news category  \n",
    "model = multiclass_classifier()\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "trained_model, train_acc, train_loss = train_model(model, loss_fn, optimizer, 20, loader)\n",
    "validate_model(trained_model, loss_fn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SBERT embeddings as input to NN, compare results \n",
    "news_title_entity['Numerical_Category'] = cat_to_int(news_title_entity, \"category\")\n",
    "news_title_entity_ds = NewsDataset(np.vstack(news_title_entity.title_embedding), news_title_entity['Numerical_Category'])\n",
    "sbert_train_dataset, sbert_test_dataset = test_training_split(news_title_entity_ds, 0.8)\n",
    "sbert_loader = DataLoader(sbert_train_dataset, shuffle=True, batch_size = 10)\n",
    "sbert_test_loader = DataLoader(sbert_test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 6477.412103056908, training accuracy: 0.6004241704940796\n",
      "epoch: 1, training loss: 6238.686894536018, training accuracy: 0.7168731689453125\n",
      "epoch: 2, training loss: 6199.517315745354, training accuracy: 0.7342782020568848\n",
      "epoch: 3, training loss: 6173.627158045769, training accuracy: 0.7369041442871094\n",
      "epoch: 4, training loss: 6151.771329283714, training accuracy: 0.7405399680137634\n",
      "epoch: 5, training loss: 6133.178672194481, training accuracy: 0.7431322336196899\n",
      "epoch: 6, training loss: 6117.650912761688, training accuracy: 0.7486533522605896\n",
      "epoch: 7, training loss: 6102.00263261795, training accuracy: 0.748788058757782\n",
      "epoch: 8, training loss: 6087.744012594223, training accuracy: 0.7533329129219055\n",
      "epoch: 9, training loss: 6074.91376376152, training accuracy: 0.7537368535995483\n",
      "epoch: 10, training loss: 6063.97824037075, training accuracy: 0.7557231187820435\n",
      "epoch: 11, training loss: 6053.770397186279, training accuracy: 0.7590223550796509\n",
      "epoch: 12, training loss: 6044.478383421898, training accuracy: 0.7603690028190613\n",
      "epoch: 13, training loss: 6037.132388472557, training accuracy: 0.7626919150352478\n",
      "epoch: 14, training loss: 6029.778399586678, training accuracy: 0.7662267684936523\n",
      "epoch: 15, training loss: 6022.63530421257, training accuracy: 0.7671694159507751\n",
      "epoch: 16, training loss: 6016.580573558807, training accuracy: 0.7717142701148987\n",
      "epoch: 17, training loss: 6010.839537262917, training accuracy: 0.7730271816253662\n",
      "epoch: 18, training loss: 6006.158699631691, training accuracy: 0.7739361524581909\n",
      "epoch: 19, training loss: 6001.150896072388, training accuracy: 0.7762590646743774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7508), 15643.966765522957)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sbert model; out performs mean pooled entity embeddings \n",
    "model = multiclass_classifier_title()\n",
    "loss_fn = nn.CrossEntropyLoss(weight = weights) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "trained_model_sbert, train_acc, train_loss = train_model(model, loss_fn, optimizer, 20, sbert_loader)\n",
    "validate_model(trained_model_sbert, loss_fn, sbert_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train simple NN with different set of inputs: concatenate pooled entity vectors with SBERT embedding  of title\n",
    "concat_embeddings = np.concatenate((np.vstack(news_title_entity.title_embedding), np.vstack(news_title_entity.entity_vec)), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 6407.796445846558, training accuracy: 0.6763062477111816\n",
      "epoch: 1, training loss: 6201.842008829117, training accuracy: 0.7361298203468323\n",
      "epoch: 2, training loss: 6158.539595723152, training accuracy: 0.75026935338974\n",
      "epoch: 3, training loss: 6129.76941382885, training accuracy: 0.7564637660980225\n",
      "epoch: 4, training loss: 6106.763685464859, training accuracy: 0.7617155909538269\n",
      "epoch: 5, training loss: 6088.338180184364, training accuracy: 0.7645098567008972\n",
      "epoch: 6, training loss: 6071.910728812218, training accuracy: 0.767876386642456\n",
      "epoch: 7, training loss: 6057.455435872078, training accuracy: 0.7709062695503235\n",
      "epoch: 8, training loss: 6045.088632583618, training accuracy: 0.7751481533050537\n",
      "epoch: 9, training loss: 6033.756937980652, training accuracy: 0.7762254476547241\n",
      "epoch: 10, training loss: 6025.061386466026, training accuracy: 0.7796929478645325\n",
      "epoch: 11, training loss: 6014.832895755768, training accuracy: 0.7798949480056763\n",
      "epoch: 12, training loss: 6007.559971451759, training accuracy: 0.7831605076789856\n",
      "epoch: 13, training loss: 6000.807894825935, training accuracy: 0.7853487730026245\n",
      "epoch: 14, training loss: 5993.758617758751, training accuracy: 0.788883626461029\n",
      "epoch: 15, training loss: 5989.003612875938, training accuracy: 0.7908025979995728\n",
      "epoch: 16, training loss: 5982.596705913544, training accuracy: 0.7935295104980469\n",
      "epoch: 17, training loss: 5978.175594091415, training accuracy: 0.7938324809074402\n",
      "epoch: 18, training loss: 5973.352137565613, training accuracy: 0.7954820990562439\n",
      "epoch: 19, training loss: 5969.453849315643, training accuracy: 0.7983773350715637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7731), 15520.653519511223)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting category with concatenated embeddings (entity; title sbert) provides best training, test accuracy, hooray!\n",
    "news_title_entity_ds = NewsDataset(concat_embeddings, news_title_entity['Numerical_Category'])\n",
    "concat_train_dataset, concat_test_dataset = test_training_split(news_title_entity_ds, 0.8)\n",
    "concat_loader = DataLoader(concat_train_dataset, shuffle = True, batch_size = 10)\n",
    "concat_test_loader = DataLoader(concat_test_dataset, shuffle = True)\n",
    "\n",
    "model = multiclass_classifier_concat()\n",
    "loss_fn = nn.CrossEntropyLoss(weight = weights) \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "trained_model_concat, train_acc, train_loss = train_model(model, loss_fn, optimizer, 20, concat_loader)\n",
    "validate_model(trained_model_concat, loss_fn, concat_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample_proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
